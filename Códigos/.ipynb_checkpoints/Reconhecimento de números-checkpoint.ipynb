{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f721cbf",
   "metadata": {},
   "source": [
    "# Construção de uma rede neural e treinamento com dados retirados do Kaggle\n",
    "\n",
    "* Repositório: https://www.kaggle.com/datasets/devanshusingh/hand-written-digits-for-object-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cbf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.image as image\n",
    "from time import time\n",
    "import imageio as io\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c43aa3",
   "metadata": {},
   "source": [
    "# Funções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a82708bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, y, b, val):    #Função de ativação usada para avaliar a rede\n",
    "\n",
    "    z = np.dot(y, x)\n",
    "    \n",
    "    if val:\n",
    "        z = z.transpose() + b\n",
    "    else:\n",
    "        z += b\n",
    "\n",
    "    a = (1/(1+np.exp(-z)))\n",
    "            \n",
    "    #a = np.array(a)\n",
    "\n",
    "    return a, z\n",
    "\n",
    "def softmax(x, y, b, val):\n",
    "\n",
    "    z = np.dot(y, x)\n",
    "    \n",
    "    if val:\n",
    "        output = []\n",
    "        z = z.transpose() + b\n",
    "        for exp in np.exp(z):\n",
    "            output.append(exp / np.sum(exp))\n",
    "        np.array(output)\n",
    "    else:\n",
    "        z += b\n",
    "        exp = np.exp(z)\n",
    "        output = exp / np.sum(exp)\n",
    "\n",
    "    return output, z\n",
    "\n",
    "def tanh(x, y, b):\n",
    "    z = np.dot(y, x) + b\n",
    "    a = []\n",
    "    for h in z:\n",
    "        if h >= 700:\n",
    "            a.append(1)\n",
    "\n",
    "        elif h <= -700:\n",
    "            a.append(0)\n",
    "\n",
    "        else:\n",
    "            a.append(np.tanh(h))\n",
    "            \n",
    "    a = np.array(a)\n",
    "\n",
    "    return a, z\n",
    "\n",
    "def derivate_sigmoid(X):  #Derivada da função de ativação\n",
    "\n",
    "    exp = np.exp(-X)\n",
    "    \n",
    "    der = (1/(1 + exp) * (1 - 1/(1 + exp)))\n",
    "\n",
    "                       \n",
    "    return der\n",
    "\n",
    "\n",
    "def derivate_tanh(X):  #Derivada da função de ativação\n",
    "    der = []\n",
    "    for x in X:\n",
    "        \n",
    "        if x < 700 and x > -700:   # Condição usada para não dar erro, mas espera-se que os valores nunca estejam fora desse intervalo, já que o neurônio ficaria saturado\n",
    "            der.append(1-np.tanh(x)**2)\n",
    "                       \n",
    "        else:\n",
    "            der.append(0)\n",
    "                       \n",
    "    return np.array(der)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, architecture):   # Aqui, serão iniciados aleatóriamente os parâmetros da rede, conforme estrutura informada em \"architeture\"\n",
    "        self.architecture = architecture\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        for i in np.arange(1,len(architecture)):\n",
    "            self.w.append([])\n",
    "            self.b.append([])\n",
    "            for j in range(architecture[i]):\n",
    "                self.b[i-1].append(np.random.normal(0,1))\n",
    "                self.w[i-1].append([])\n",
    "                for k in range(architecture[i-1]):\n",
    "                    self.w[i-1][j].append(np.random.normal(0,1/np.sqrt(architecture[i-1])))  # Este desvio padrão é usado para previnir que os neurônios estejam saturados no começo\n",
    "            self.w[i-1] = np.array(self.w[i-1])\n",
    "            self.b[i-1] = np.array(self.b[i-1])\n",
    "            \n",
    "    def cost_function(self, x, y):    # x -> output; y -> input \n",
    "        C = 0\n",
    "        n = len(y)\n",
    "        a, trash = self.evaluate_2(np.array(x))\n",
    "        a = a[-1]\n",
    "        \n",
    "        C += np.sum((np.array(y) - np.array(a))**2)/n #-np.sum(np.array(y) * np.log(np.array(a)))/n\n",
    "                #C += -(x[i][j] * np.log(a[j]) + (1-x[i][j]) * np.log(1-a[j]))/n\n",
    "\n",
    "        return C\n",
    "    \n",
    "    def cost_function_g(self, x, y):    # x -> output; y -> input \n",
    "        C = 0\n",
    "        n = len(y)\n",
    "        a, trash = self.evaluate_2(np.array(x))\n",
    "        a = a[-1]\n",
    "        \n",
    "        C += np.sum(np.abs(np.array(y) - np.array(a)))/n #-np.sum(np.array(y) * np.log(np.array(a)))/n\n",
    "                #C += -(x[i][j] * np.log(a[j]) + (1-x[i][j]) * np.log(1-a[j]))/n\n",
    "\n",
    "        return C\n",
    "\n",
    "\n",
    "    def evaluate(self, y):      # Avalia a rede neural\n",
    "        a = [y]\n",
    "        z = []\n",
    "        for i in range(len(self.w)):\n",
    "            \n",
    "            if i == len(self.w) - 1:\n",
    "                vec_aux = softmax(a[i], self.w[i], self.b[i], False)\n",
    "            else:\n",
    "                vec_aux = sigmoid(a[i], self.w[i], self.b[i], False)\n",
    "                \n",
    "            z.append(vec_aux[1])                  \n",
    "            a.append(vec_aux[0])\n",
    "\n",
    "            \n",
    "        return a, z                # A saída mais importante é o \"a\", mas o \"z\" foi acrescentado aqui em uma tentativa de otimização\n",
    "    \n",
    "    \n",
    "    def evaluate_2(self, y):      # Avalia a rede neural\n",
    "        a = [y]\n",
    "        z = []\n",
    "        for i in range(len(self.w)):\n",
    "            \n",
    "            if i == len(self.w) - 1:\n",
    "                vec_aux = softmax(a[i].transpose(), self.w[i], self.b[i], True)\n",
    "            else:\n",
    "                vec_aux = sigmoid(a[i].transpose(), self.w[i], self.b[i], True)\n",
    "                \n",
    "            z.append(vec_aux[1])                  \n",
    "            a.append(vec_aux[0])\n",
    "\n",
    "            \n",
    "        return a, z               \n",
    "\n",
    "\n",
    "    def update(self, y, x, eta9, nu):           # Função usada para atualizar os pesos, W's, e bias, b's \n",
    "        \n",
    "        eta = -eta9  # Learning rate\n",
    "        \n",
    "        #dw, db = derivative(y, self, x, self.w, self.b)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        n = len(y)\n",
    "\n",
    "\n",
    "        dw = []\n",
    "        db = []\n",
    "        \n",
    "        for i in range(len(self.w)):\n",
    "            dw.append(0)\n",
    "            db.append(0)\n",
    "\n",
    "        a, z = self.evaluate_2(np.array(y))  # Avalia os dados de entrada na rede e devolve os dados de saída, \"a\", e o \"z\", que é usado no processo de avaliação    \n",
    "                \n",
    "        for l in range(n): #while l < n:           \n",
    "            \n",
    "            for i in range(len(self.w)):   # Rodando em todas as camadas da rede\n",
    "                \n",
    "                i = len(self.w) - i - 1    # O algorítmo do 'Back Propagation' começa no fim e termina no começo\n",
    "                \n",
    "                da_aux = 0\n",
    "\n",
    "\n",
    "                d_sigma = derivate_sigmoid(z[i][l]) \n",
    "\n",
    "  \n",
    "                '''\n",
    "                ---------------------------------------------------------------------------------------------------------------------\n",
    "                Aqui é calculado a derivada da função de custo em relação aos a's - matriz representando a saída de cada neurônio da rede\n",
    "                \n",
    "                '''\n",
    "\n",
    "                if i == len(self.w) - 1 :\n",
    "                    \n",
    "                    da_aux = np.dot(a[i + 1][l] - x[l], self.w[i])\n",
    "                        \n",
    "                else:\n",
    "                    \n",
    "                    da_aux = np.dot(da * d_sigma, self.w[i])\n",
    "                    \n",
    "                '''\n",
    "                ---------------------------------------------------------------------------------------------------------------------\n",
    "                Aqui é calculado a derivada da função de custo em relação aos w's - pesos dos neurônios\n",
    "                \n",
    "                '''\n",
    "                        \n",
    "\n",
    "                if i == len(self.w) - 1 :\n",
    "                    \n",
    "                    dw_aux = np.outer(a[i + 1][l] - x[l], a[i][l]) + nu * self.w[i]\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    dw_aux = np.outer(da * d_sigma, a[i][l]) + nu * self.w[i] \n",
    "                    \n",
    "                '''\n",
    "                ---------------------------------------------------------------------------------------------------------------------\n",
    "                Aqui é calculado a derivada da função de custo em relação aos b's\n",
    "                \n",
    "                '''\n",
    "\n",
    "                if i == len(self.w) - 1:              \n",
    "\n",
    "                    db_aux = a[i + 1][l] - x[l]           \n",
    "\n",
    "                else:      \n",
    "                    db_aux = da * d_sigma\n",
    "                    \n",
    "                # O resultado é, então, acresentado em dw e db, representando a derivada total na época em questão\n",
    "\n",
    "                dw[i] += eta * np.array(dw_aux)\n",
    "                db[i] += eta * np.array(db_aux)\n",
    "                da = da_aux\n",
    "  \n",
    "        #------------------------------------------------------------------------------------------------\n",
    "            \n",
    "        for m in range(len(self.w)):   # Os w's e b's são atualizados\n",
    "            \n",
    "            self.w[m] += dw[m] / n\n",
    "            self.b[m] += db[m] / n\n",
    "\n",
    "                    \n",
    "        return None\n",
    "    \n",
    "    def run_epoch(self, times_interaction, Training_input, training_output):    \n",
    "        eta = 1                     # Taxa de aprendizagem (Learning rate)\n",
    "        nu = 0.0005                    # Regularização\n",
    "        graph = False              # Informa se será plotado gráficos da função de custo\n",
    "        minibatch = 200            # length of the minibatch. If 0, all the integer batch is going to be used\n",
    "\n",
    "        if minibatch != 0:    # Necessário para fazer o random choice\n",
    "            dataset = {}\n",
    "            dataset['input'] = Training_input\n",
    "            dataset['output'] = training_output\n",
    "            dataset = pd.DataFrame(dataset)\n",
    "            index = np.arange(0,len(Training_input)).tolist()\n",
    "\n",
    "        if graph:             # Dados para o ponto de interação zero do gráfico\n",
    "\n",
    "            Training_cost = [self.cost_function(training_output, Training_input)]       \n",
    "            Validation_cost = [self.cost_function(validation_output, validation_input)]\n",
    "\n",
    "        start = time()\n",
    "        for i in range(times_interaction):\n",
    "\n",
    "            if minibatch != 0:  #Random choice do minibatch\n",
    "\n",
    "                chose_index = random.sample(index, minibatch)\n",
    "                Training_input_aux = dataset.loc[chose_index, 'input'].values.tolist()\n",
    "                training_output_aux = dataset.loc[chose_index, 'output'].values.tolist()\n",
    "                self.update(Training_input_aux, training_output_aux, eta, nu)\n",
    "\n",
    "            else:    \n",
    "\n",
    "                self.update(Training_input, training_output, eta, nu)\n",
    "\n",
    "            if graph:\n",
    "                Training_cost.append(self.cost_function(training_output, Training_input))\n",
    "                Validation_cost.append(self.cost_function(validation_output, validation_input))\n",
    "\n",
    "            if i == 0:           # Informa o tempo que levará para acabar\n",
    "                stop = time()\n",
    "                print(str(-start * times_interaction/3600 + stop * times_interaction/3600) + 'h para finalizar')\n",
    "\n",
    "\n",
    "        if times_interaction > 0 and graph:\n",
    "            t = np.arange(0, times_interaction+1)\n",
    "            plt.plot(t, Training_cost, '.', label = 'Training')\n",
    "            plt.plot(t, Validation_cost, '.', label = 'Validation')\n",
    "            plt.xlabel('Learning Interaction')\n",
    "            plt.ylabel('Cost Function')\n",
    "            plt.title('Cost Function Evolution')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return None\n",
    "\n",
    "def interpretation(y, x):\n",
    "    t = 0\n",
    "    length = abs(y[0] - 1)\n",
    "    for b in range(len(y)):\n",
    "        if abs(y[b] - 1) < length:\n",
    "            t = b\n",
    "            length = abs(y[b] - 1)\n",
    "            \n",
    "    k = np.zeros(10)\n",
    "    k[t] = 1\n",
    "    aux = 1\n",
    "    for i in range(len(x)):\n",
    "        if x[i] != k[i]:\n",
    "            aux = 0\n",
    "        \n",
    "    return aux\n",
    "        \n",
    "def test(net, validation_input, validation_output):\n",
    "    cont = 0\n",
    "    result_output, trash = net.evaluate_2(np.array(validation_input))\n",
    "    for i in range(len(validation_input)):\n",
    "        \n",
    "        test = interpretation(result_output[-1][i], validation_output[i])\n",
    "\n",
    "        if test:\n",
    "            cont += 1\n",
    "    return cont/len(validation_output)*100\n",
    "\n",
    "def saving(net):\n",
    "    for i in range(len(net.w)):\n",
    "        file = open(str(i) + '.txt', 'w')\n",
    "        for j in range(len(net.w[i])):\n",
    "            for k in range(len(net.w[i][j])):\n",
    "                file.write(str(net.w[i][j][k]) + ';')\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        file.close()\n",
    "    file = open('b' + '.txt', 'w')\n",
    "    for i in range(len(net.b)):\n",
    "        for j in range(len(net.b[i])):\n",
    "            file.write(str(net.b[i][j]) + ';')\n",
    "        file.write('\\n')\n",
    "    file.close()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def recovering(net):\n",
    "    for i in range(len(net.w)):\n",
    "        file = open(str(i) + '.txt', 'r')\n",
    "        line = file.readline()\n",
    "        j = 0\n",
    "        while line != '':\n",
    "            aux = ''\n",
    "            k = 0\n",
    "            for c in line:\n",
    "                if c != ';' and c != '\\n':\n",
    "                    aux += c\n",
    "                elif c != '\\n':\n",
    "                    net.w[i][j][k] = float(aux)\n",
    "                    aux = ''\n",
    "                    k += 1\n",
    "            j += 1\n",
    "            line = file.readline()\n",
    "\n",
    "\n",
    "    file = open('b' + '.txt', 'r')\n",
    "    line = file.readline()\n",
    "    i = 0\n",
    "    while line != '':\n",
    "        aux = ''\n",
    "        j = 0\n",
    "        for c in line:\n",
    "            if c != ';' and c != '\\n':\n",
    "                aux += c\n",
    "            elif c != '\\n':\n",
    "                net.b[i][j] = float(aux)\n",
    "                aux = ''\n",
    "                j += 1\n",
    "        i += 1\n",
    "        line = file.readline()\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba7341",
   "metadata": {},
   "source": [
    "* Carregando arquivos do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8692974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "Training_input = []\n",
    "training_output = []\n",
    "path = os.path.join(path, 'Data')\n",
    "path = os.path.join(path, 'training')\n",
    "\n",
    "for c in os.listdir(path):\n",
    "    \n",
    "    path2 = os.path.join(path, c)\n",
    "\n",
    "    for k in os.listdir(path2):\n",
    "        \n",
    "        im = io.imread(path2 +'\\\\'+ k)\n",
    "        aux = []\n",
    "        \n",
    "        for i in range(len(im)):\n",
    "            for j in range(len(im[i])):\n",
    "                aux.append(im[i][j]/255)\n",
    "                \n",
    "        Training_input.append(aux)\n",
    "        \n",
    "        training_output.append(np.zeros(10))\n",
    "        training_output[-1][int(c)] = 1\n",
    "        \n",
    "        \n",
    "path = os.getcwd()\n",
    "path = os.path.join(path, 'Data')\n",
    "path = os.path.join(path, 'testing')\n",
    "validation_input = []\n",
    "validation_output = []\n",
    "\n",
    "\n",
    "for c in os.listdir(path):\n",
    "    \n",
    "    path2 = os.path.join(path, c)\n",
    "\n",
    "    for k in os.listdir(path2):\n",
    "        \n",
    "        im = io.imread(path2 +'\\\\'+ k)\n",
    "        aux = []\n",
    "        \n",
    "        for i in range(len(im)):\n",
    "            for j in range(len(im[i])):\n",
    "                aux.append(im[i][j]/255)\n",
    "                \n",
    "        validation_input.append(aux)\n",
    "        \n",
    "        validation_output.append(np.zeros(10))\n",
    "        validation_output[-1][int(c)] = 1\n",
    "        \n",
    "        \n",
    "net = Network((len(validation_input[0]),101,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6efc9886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005666740238666534h para finalizar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91.47"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.run_epoch(100, Training_input, training_output)\n",
    "test(net, validation_input, validation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac0b14",
   "metadata": {},
   "source": [
    "# A partir daqui, há blocos de códigos para algoritmos genéticos e testes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e75182",
   "metadata": {},
   "source": [
    "* Criando redes neurais para usar no algorítimo genético e ordenando-as por eficiência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c63e36f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8006507556681024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0468606575060877"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Network((len(validation_input[0]),30,10))\n",
    "print(net.cost_function_g(Training_input, training_output))\n",
    "net.cost_function(Training_input, training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41ef5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "num_net = 20\n",
    "testing = []\n",
    "for j in range(num_net):\n",
    "    nets.append(Network((len(validation_input[0]), 101, 10)))\n",
    "    \n",
    "    testing.append(nets[j].cost_function(Training_input, training_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d184ca2",
   "metadata": {},
   "source": [
    "* Genetic algorithm training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e60ae09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.947999477386475 min para finalizar\n"
     ]
    }
   ],
   "source": [
    "def mutating(net, cost):\n",
    "    mutation = 0.01 * (1 - cost / 10)\n",
    "    for i in range(len(net.w)):\n",
    "        for j in range(len(net.w[i])):\n",
    "            for k in range(len(net.w[i][j])):\n",
    "                if random.random() < mutation:\n",
    "                    net.w[i][j][j] = np.random.normal(net.w[i][j][j], 1/np.sqrt(len(net.w[i][j])))\n",
    "\n",
    "    return None\n",
    "            \n",
    "def crossing(net_1, net_2):\n",
    "    \n",
    "    net = Network((len(validation_input[0]), 101, 10))\n",
    "    \n",
    "    \n",
    "    for i in range(len(net.w)):\n",
    "        #trashold_b = random.randint(0, len(net.b[i]) - 1)\n",
    "        net.w[i] = (net_1.w[i] + net_2.w[i]) / 2\n",
    "\n",
    "        net.b[i] = (net_1.b[i] + net_2.b[i]) / 2\n",
    "    \n",
    "    cost = net.cost_function(Training_input, training_output)\n",
    "        \n",
    "    mutating(net, cost)\n",
    "        \n",
    "        \n",
    "    return net, cost\n",
    "      \n",
    "times_interaction = 10\n",
    "num_net = len(nets)\n",
    "start = time()\n",
    "n = len(nets)\n",
    "\n",
    "for j in range(times_interaction):\n",
    "    prob = 1 / np.array(testing)\n",
    "    prob = prob / np.sum(prob)\n",
    "    nets_aux = []\n",
    "    testing_aux = []\n",
    "    \n",
    "    for k in range(n):\n",
    "        index = np.arange(0,len(nets)).tolist()\n",
    "        net_1_index = np.random.choice(index, p = prob)\n",
    "        net_1 = nets[net_1_index]\n",
    "        index.remove(net_1_index)\n",
    "        prob_aux = np.array([prob[x] for x in range(len(prob)) if x != net_1_index])\n",
    "        prob_aux = prob_aux / np.sum(prob_aux)\n",
    "        net_2_index = np.random.choice(index, p = prob_aux)\n",
    "        net_2 = nets[net_2_index]\n",
    "        net, cost = crossing(net_1, net_2)\n",
    "        testing_aux.append(cost)\n",
    "        nets_aux.append(net)\n",
    "        \n",
    "    nets = nets_aux * 1\n",
    "    testing = testing_aux * 1\n",
    "    \n",
    "    if j == 0:\n",
    "        stop = time()\n",
    "\n",
    "        print(str((stop - start) * times_interaction / 60) + ' min para finalizar')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188fbde",
   "metadata": {},
   "source": [
    "* Testando o conjunto de redes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f10322",
   "metadata": {},
   "source": [
    "# Trainamento e teste de dados com a biblioteca SKLearning\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ebf779f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic', alpha=0, hidden_layer_sizes=30,\n",
       "             learning_rate_init=0.05, max_iter=300, solver='sgd')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna = MLPRegressor(hidden_layer_sizes = (30), verbose = False, activation = 'logistic', solver = 'sgd', alpha = 0, learning_rate = 'constant', learning_rate_init = 0.05, tol = 0.0001, max_iter = 300)\n",
    "rna.fit(Training_input, training_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5dabda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.89% de acertos\n"
     ]
    }
   ],
   "source": [
    "cont = 0\n",
    "for i in range(len(validation_input)):\n",
    "    result_output = rna.predict([validation_input[i]])\n",
    "    test = interpretation(result_output[0], validation_output[i])\n",
    "    if test:\n",
    "        cont += 1\n",
    "print(str(cont/len(validation_output)*100) + '% de acertos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9c6d0",
   "metadata": {},
   "source": [
    "# Relatos\n",
    "\n",
    "* Network with one haden layer and 28 neurons, learning rate of 0.005 and regularization parameter of 0.0001\n",
    "\n",
    "     95.6% of success, with approximately 4500 epoches\n",
    "     \n",
    "     \n",
    "* Network with one haden layer and 101 neurons, learning rate of 0.005 and regularization parameter of 0.0005\n",
    "\n",
    "     96.13% of success"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
